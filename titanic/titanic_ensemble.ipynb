{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631e8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukeb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    RandomizedSearchCV,\n",
    "    GridSearchCV,\n",
    "    RepeatedKFold\n",
    ")\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, accuracy_score\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cbae9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 712 entries, 81 to 385\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   Pclass           712 non-null    int64   \n",
      " 1   Sex              712 non-null    object  \n",
      " 2   Age              712 non-null    float64 \n",
      " 3   Fare             712 non-null    float64 \n",
      " 4   Embarked         712 non-null    object  \n",
      " 5   family_size_cat  712 non-null    category\n",
      "dtypes: category(1), float64(2), int64(1), object(2)\n",
      "memory usage: 34.3+ KB\n",
      "None\n",
      "           Pclass         Age        Fare\n",
      "count  712.000000  712.000000  712.000000\n",
      "mean     2.327247   29.659892   32.197050\n",
      "std      0.834178   12.860939   51.249828\n",
      "min      1.000000    0.420000    0.000000\n",
      "25%      2.000000   22.000000    7.895800\n",
      "50%      3.000000   29.699118   13.895850\n",
      "75%      3.000000   35.000000   31.068750\n",
      "max      3.000000   80.000000  512.329200\n",
      "         Sex Embarked\n",
      "count    712      712\n",
      "unique     2        3\n",
      "top     male        S\n",
      "freq     465      520\n",
      "Survived\n",
      "0    0.616573\n",
      "1    0.383427\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Prep data\n",
    "X = pd.read_csv('train.csv')\n",
    "X_final = pd.read_csv('test.csv')\n",
    "\n",
    "#save passengerId for submission\n",
    "pid = X_final['PassengerId']\n",
    "\n",
    "y = X['Survived']\n",
    "X.drop(['Survived'], axis=1, inplace=True)\n",
    "\n",
    "#drop variables\n",
    "#   Cabin - too many missing values\n",
    "#   PassengerId - not relevant\n",
    "#   Name - not relevant\n",
    "#   Ticket - not relevant\n",
    "X.drop(['Cabin'], axis=1, inplace=True)\n",
    "X.drop(['PassengerId'], axis=1, inplace=True)\n",
    "X.drop(['Name'], axis=1, inplace=True)\n",
    "X.drop(['Ticket'], axis=1, inplace=True)\n",
    "\n",
    "X_final.drop(['Cabin'], axis=1, inplace=True)\n",
    "X_final.drop(['PassengerId'], axis=1, inplace=True)\n",
    "X_final.drop(['Name'], axis=1, inplace=True)\n",
    "X_final.drop(['Ticket'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#impute Age and Embarked\n",
    "X['Age'] = X['Age'].fillna(X['Age'].mean())\n",
    "X['Embarked'] = X['Embarked'].fillna('S')\n",
    "X_final['Age'] = X_final['Age'].fillna(X_final['Age'].mean())\n",
    "X_final['Embarked'] = X_final['Embarked'].fillna('S')\n",
    "\n",
    "#Impute Fare - missing Fare values in final test\n",
    "X_final['Fare'] = X_final['Fare'].fillna(X_final['Fare'].mean())\n",
    "\n",
    "\n",
    "\n",
    "#Feature Engineering\n",
    "\n",
    "#create family size continuous variable\n",
    "X['family_size'] = X['Parch'] + X['SibSp']\n",
    "X_final['family_size'] = X_final['Parch'] + X_final['SibSp']\n",
    "\n",
    "bins = [0, 0.1, 3, 6, 20]\n",
    "labels = ['None', 'Small', 'Medium', 'Large']\n",
    "\n",
    "#create the categorical variable\n",
    "X['family_size_cat'] = pd.cut(X['family_size'], bins=bins, labels=labels, right=False)\n",
    "X_final['family_size_cat'] = pd.cut(X_final['family_size'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "#drop duplicate vars\n",
    "X.drop(['SibSp'], axis=1, inplace=True)\n",
    "X.drop(['Parch'], axis=1, inplace=True)\n",
    "X.drop(['family_size'], axis=1, inplace=True)\n",
    "\n",
    "X_final.drop(['SibSp'], axis=1, inplace=True)\n",
    "X_final.drop(['Parch'], axis=1, inplace=True)\n",
    "X_final.drop(['family_size'], axis=1, inplace=True)\n",
    "\n",
    "#Get train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "print(X_train.info())\n",
    "print(X_train.describe())\n",
    "print(X_train.describe(include='object'))\n",
    "\n",
    "print(y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f19f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode categorical features\n",
    "enc = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",   # Allow unseen categories during transform\n",
    "    unknown_value=-1,                     # Code for unseen categories\n",
    "    encoded_missing_value=-2,             # Code for missing values (NaN)\n",
    "    dtype=np.int64                        \n",
    ")\n",
    "\n",
    "#Get categorical features\n",
    "cat_cols = X_train.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "\n",
    "#Encode training set, test set, and final set\n",
    "X_train[cat_cols] = enc.fit_transform(X_train[cat_cols])\n",
    "X_test[cat_cols] = enc.fit_transform(X_test[cat_cols])\n",
    "X_final[cat_cols] = enc.fit_transform(X_final[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91000c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "Execution Time: 00:00:02\n",
      "Average cross-validation accuracy: 0.8103870777110214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       106\n",
      "           1       0.75      0.70      0.72        73\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.77      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Logistic Regression\n",
    "\n",
    "# Track time\n",
    "start = time.time()\n",
    "\n",
    "# define model\n",
    "baseline_logistic_regression = LogisticRegression() # leave max_iter at default (100) and ignore warning - increasing takes too long\n",
    "\n",
    "# fit model\n",
    "baseline_logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set and generate classification report\n",
    "baseline_logistic_regression_preds = baseline_logistic_regression.predict(X_test)\n",
    "baseline_logistic_regression_report = classification_report(y_test, baseline_logistic_regression_preds)\n",
    "\n",
    "# get accuracy scores\n",
    "baseline_logistic_regression_accs = cross_val_score(baseline_logistic_regression, X_train, y_train, scoring='accuracy', cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), n_jobs=-1)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# print outputs\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))\n",
    "print(f\"Average cross-validation accuracy: {baseline_logistic_regression_accs.mean()}\")\n",
    "print(baseline_logistic_regression_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28183bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Classification\n",
      "Execution Time: 00:00:00\n",
      "Average cross-validation accuracy: 0.8050586033684624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81       106\n",
      "           1       0.74      0.67      0.71        73\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.77      0.76      0.76       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Linear Support Vector Classification\n",
    "\n",
    "# Track time\n",
    "start = time.time()\n",
    "\n",
    "# define model\n",
    "baseline_lsvc = LinearSVC()\n",
    "\n",
    "# fit model\n",
    "baseline_lsvc.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set and generate classification report\n",
    "baseline_lsvc_preds = baseline_lsvc.predict(X_test)\n",
    "baseline_lsvc_report = classification_report(y_test, baseline_lsvc_preds)\n",
    "\n",
    "# get accuracy scores\n",
    "baseline_lsvc_accs = cross_val_score(baseline_lsvc, X_train, y_train, scoring='accuracy', cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), n_jobs=-1) # lower repeats if it takes too long\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# print outputs\n",
    "print(\"Linear Support Vector Classification\")\n",
    "print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))\n",
    "print(f\"Average cross-validation accuracy: {baseline_lsvc_accs.mean()}\")\n",
    "print(baseline_lsvc_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8859e228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Execution Time: 00:00:00\n",
      "Average cross-validation accuracy: 0.7783610755441741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78       106\n",
      "           1       0.68      0.66      0.67        73\n",
      "\n",
      "    accuracy                           0.73       179\n",
      "   macro avg       0.72      0.72      0.72       179\n",
      "weighted avg       0.73      0.73      0.73       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Decision Tree\n",
    "\n",
    "# Track time\n",
    "start = time.time()\n",
    "\n",
    "# define model\n",
    "baseline_decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# fit model\n",
    "baseline_decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set and generate classification report\n",
    "baseline_decision_tree_preds = baseline_decision_tree.predict(X_test)\n",
    "baseline_decision_tree_report = classification_report(y_test, baseline_decision_tree_preds)\n",
    "\n",
    "# get accuracy scores\n",
    "baseline_decision_tree_accs = cross_val_score(baseline_decision_tree, X_train, y_train, scoring='accuracy', cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), n_jobs=-1)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# print outputs\n",
    "print(\"Decision Tree\")\n",
    "print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))\n",
    "print(f\"Average cross-validation accuracy: {baseline_decision_tree_accs.mean()}\")\n",
    "print(baseline_decision_tree_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d22b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Execution Time: 00:00:00\n",
      "Average cross-validation accuracy: 0.7955244755244755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84       106\n",
      "           1       0.76      0.75      0.76        73\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.80      0.80       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Random Forest\n",
    "\n",
    "# Track time\n",
    "start = time.time()\n",
    "\n",
    "# define model\n",
    "baseline_random_forest = RandomForestClassifier()\n",
    "\n",
    "# fit model\n",
    "baseline_random_forest.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set and generate classification report\n",
    "baseline_random_forest_preds = baseline_random_forest.predict(X_test)\n",
    "baseline_random_forest_report = classification_report(y_test, baseline_random_forest_preds)\n",
    "\n",
    "# get accuracy scores\n",
    "baseline_random_forest_accs = cross_val_score(baseline_random_forest, X_train, y_train, scoring='accuracy', cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), n_jobs=-1)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# print outputs\n",
    "print(\"Random Forest\")\n",
    "print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))\n",
    "print(f\"Average cross-validation accuracy: {baseline_random_forest_accs.mean()}\")\n",
    "print(baseline_random_forest_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55776b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier\n",
      "Execution Time: 00:00:00\n",
      "Average cross-validation accuracy: 0.8216586230670738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       106\n",
      "           1       0.82      0.73      0.77        73\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.82      0.81      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Gradient Boosting Classifier\n",
    "\n",
    "# Track time\n",
    "start = time.time()\n",
    "\n",
    "# define model\n",
    "baseline_gbc = GradientBoostingClassifier()\n",
    "\n",
    "# fit model\n",
    "baseline_gbc.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set and generate classification report\n",
    "baseline_gbc_preds = baseline_gbc.predict(X_test)\n",
    "baseline_gbc_report = classification_report(y_test, baseline_gbc_preds)\n",
    "\n",
    "# get accuracy scores\n",
    "baseline_gbc_accs = cross_val_score(baseline_gbc, X_train, y_train, scoring='accuracy', cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), n_jobs=-1)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# print outputs\n",
    "print(\"Gradient Boosting Classifier\")\n",
    "print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))\n",
    "print(f\"Average cross-validation accuracy: {baseline_gbc_accs.mean()}\")\n",
    "print(baseline_gbc_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d073ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Classifier\n",
      "Execution Time: 00:00:00\n",
      "Average cross-validation accuracy: 0.7118034078597459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.73       106\n",
      "           1       0.61      0.64      0.63        73\n",
      "\n",
      "    accuracy                           0.69       179\n",
      "   macro avg       0.68      0.68      0.68       179\n",
      "weighted avg       0.69      0.69      0.69       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline K-Nearest Neighbors Classifier\n",
    "\n",
    "# Track time\n",
    "start = time.time()\n",
    "\n",
    "# define model\n",
    "baseline_knn = KNeighborsClassifier()\n",
    "\n",
    "# fit model\n",
    "baseline_knn.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set and generate classification report\n",
    "baseline_knn_preds = baseline_knn.predict(X_test)\n",
    "baseline_knn_report = classification_report(y_test, baseline_knn_preds)\n",
    "\n",
    "# get accuracy scores\n",
    "baseline_knn_accs = cross_val_score(baseline_knn, X_train, y_train, scoring='accuracy', cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), n_jobs=-1, error_score='raise')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# print outputs\n",
    "print(\"K-Nearest Neighbors Classifier\")\n",
    "print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))\n",
    "print(f\"Average cross-validation accuracy: {baseline_knn_accs.mean()}\")\n",
    "print(baseline_knn_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "333dbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "#Use optuna to find best hyperparameters\n",
    "\n",
    "#filter output\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "#define objective\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter space\n",
    "    params = {\n",
    "        'penalty'       : trial.suggest_categorical('penalty', [None, 'l2']),\n",
    "        'solver'        : trial.suggest_categorical('solver', ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag']),\n",
    "        'C'             : trial.suggest_float('C', 0.25, 3),\n",
    "        'max_iter'      : trial.suggest_int('max_iter', 100, 1000)\n",
    "    }\n",
    "\n",
    "    #create model\n",
    "    optuna_model = LogisticRegression()\n",
    "    optuna_model.set_params(**params)\n",
    "\n",
    "    #perform cross validation\n",
    "    score = cross_val_score(\n",
    "        optuna_model, X_train, y_train,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1, #something broken about the warnings when setting n_jobs=-1\n",
    "        cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5),\n",
    "        error_score='raise' #show errors\n",
    "    ).mean() #get mean accuracy score\n",
    "\n",
    "    return(score)\n",
    "\n",
    "#create optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "#store best values\n",
    "lr_best_params = study.best_params\n",
    "lr_best_cv_score = study.best_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30312eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Support Vector Classifier\n",
    "#Use optuna to find best hyperparameters\n",
    "\n",
    "#filter output\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "#define objective\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter space\n",
    "    params = {\n",
    "        'penalty'       : trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'C'             : trial.suggest_float('C', 0.25, 4),\n",
    "        'max_iter'      : trial.suggest_int('max_iter', 100, 1500)\n",
    "    }\n",
    "\n",
    "    #create model\n",
    "    optuna_model = LinearSVC()\n",
    "    optuna_model.set_params(**params)\n",
    "\n",
    "    #perform cross validation\n",
    "    score = cross_val_score(\n",
    "        optuna_model, X_train, y_train,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1, #something broken about the warnings when setting n_jobs=-1\n",
    "        cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5),\n",
    "        error_score='raise' #show errors\n",
    "    ).mean() #get mean accuracy score\n",
    "\n",
    "    return(score)\n",
    "\n",
    "#create optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "#store best values\n",
    "lsvc_best_params = study.best_params\n",
    "lsvc_best_cv_score = study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fce9d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "#Use optuna to find best hyperparameters\n",
    "\n",
    "#filter output\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "#define objective\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter space\n",
    "    params = {\n",
    "        'criterion'         : trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'splitter'          : trial.suggest_categorical('splitter', ['best', 'random']),\n",
    "        'max_depth'         : trial.suggest_int('max_depth', 2, 100),\n",
    "        'min_samples_split' : trial.suggest_int('min_samples_split', 2, 50),\n",
    "        'min_samples_leaf'  : trial.suggest_int('min_samples_leaf', 1, 150),\n",
    "        'max_features'      : trial.suggest_int('max_features', 2, 20)\n",
    "    }\n",
    "\n",
    "    #create model\n",
    "    optuna_model = DecisionTreeClassifier()\n",
    "    optuna_model.set_params(**params)\n",
    "\n",
    "    #perform cross validation\n",
    "    score = cross_val_score(\n",
    "        optuna_model, X_train, y_train,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1, #something broken about the warnings when setting n_jobs=-1\n",
    "        cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5),\n",
    "        error_score='raise' #show errors\n",
    "    ).mean() #get mean accuracy score\n",
    "\n",
    "    return(score)\n",
    "\n",
    "#create optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "#store best values\n",
    "dt_best_params = study.best_params\n",
    "dt_best_cv_score = study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "319f19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "#Use optuna to find best hyperparameters\n",
    "\n",
    "#filter output\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "#define objective\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter space\n",
    "    params = {\n",
    "        'criterion'         : trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'max_depth'         : trial.suggest_int('max_depth', 2, 100),\n",
    "        'min_samples_split' : trial.suggest_int('min_samples_split', 2, 50),\n",
    "        'min_samples_leaf'  : trial.suggest_int('min_samples_leaf', 1, 150),\n",
    "        'max_features'      : trial.suggest_int('max_features', 2, 20)\n",
    "    }\n",
    "\n",
    "    #create model\n",
    "    optuna_model = RandomForestClassifier()\n",
    "    optuna_model.set_params(**params)\n",
    "\n",
    "    #perform cross validation\n",
    "    score = cross_val_score(\n",
    "        optuna_model, X_train, y_train,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1, #something broken about the warnings when setting n_jobs=-1\n",
    "        cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5),\n",
    "        error_score='raise' #show errors\n",
    "    ).mean() #get mean accuracy score\n",
    "\n",
    "    return(score)\n",
    "\n",
    "#create optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "#store best values\n",
    "rf_best_params = study.best_params\n",
    "rf_best_cv_score = study.best_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56945c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Classifier\n",
    "#Use optuna to find best hyperparameters\n",
    "\n",
    "#filter output\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "#define objective\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter space\n",
    "    params = {\n",
    "        'loss'              : trial.suggest_categorical('loss', ['log_loss', 'exponential']),\n",
    "        'max_depth'         : trial.suggest_int('max_depth', 2, 100),\n",
    "        'min_samples_split' : trial.suggest_int('min_samples_split', 2, 50),\n",
    "        'min_samples_leaf'  : trial.suggest_int('min_samples_leaf', 1, 150),\n",
    "        'n_estimators'      : trial.suggest_int('n_estimators', 2, 500)\n",
    "    }\n",
    "\n",
    "    #create model\n",
    "    optuna_model = GradientBoostingClassifier()\n",
    "    optuna_model.set_params(**params)\n",
    "\n",
    "    #perform cross validation\n",
    "    score = cross_val_score(\n",
    "        optuna_model, X_train, y_train,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1, #something broken about the warnings when setting n_jobs=-1\n",
    "        cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5),\n",
    "        error_score='raise' #show errors\n",
    "    ).mean() #get mean accuracy score\n",
    "\n",
    "    return(score)\n",
    "\n",
    "#create optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "#store best values\n",
    "gbc_best_params = study.best_params\n",
    "gbc_best_cv_score = study.best_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7552bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Nearest Neighbors Classifier\n",
    "#Use optuna to find best hyperparameters\n",
    "\n",
    "#filter output\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "#define objective\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter space\n",
    "    params = {\n",
    "        'weights'           : trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "        'algorithm'         : trial.suggest_categorical('algorithm', ['ball_tree', 'kd_tree', 'brute']),\n",
    "        'p'                 : trial.suggest_int('p', 1, 3),\n",
    "        'n_neighbors'       : trial.suggest_int('n_neighbors', 2, 15)\n",
    "    }\n",
    "\n",
    "    #create model\n",
    "    optuna_model = KNeighborsClassifier()\n",
    "    optuna_model.set_params(**params)\n",
    "\n",
    "    #perform cross validation\n",
    "    score = cross_val_score(\n",
    "        optuna_model, X_train, y_train,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1, #something broken about the warnings when setting n_jobs=-1\n",
    "        cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5),\n",
    "        error_score='raise' #show errors\n",
    "    ).mean() #get mean accuracy score\n",
    "\n",
    "    return(score)\n",
    "\n",
    "#create optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "#store best values\n",
    "knn_best_params = study.best_params\n",
    "knn_best_cv_score = study.best_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25683a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Grid Search\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# define paramter space\n",
    "lr_param_grid = {\n",
    "    'penalty'       : [None, 'l2'],\n",
    "    'solver'        : ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag'],\n",
    "    'C'             : [0.25, 0.5, 0.75, 1, 2, 4]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "lr_grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(), \n",
    "    param_grid=lr_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5),\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get results\n",
    "lr_results_df = pd.DataFrame(lr_grid_search.cv_results_)\n",
    "\n",
    "# display top 10 models\n",
    "#lr_top10 = lr_results_df.sort_values(by='mean_test_score', ascending=True).head(10)\n",
    "\n",
    "# store best model\n",
    "lr_best_params_gs = lr_grid_search.best_params_\n",
    "lr_best_cv_score_gs = lr_grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4b906a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Support Vector Classifier Grid Search\n",
    "\n",
    "# define paramter space\n",
    "lsvc_param_grid = {\n",
    "    'penalty'       : ['l1', 'l2'],\n",
    "    'C'             : [0.25, 0.5, 0.75, 1, 2, 4],\n",
    "    'max_iter'      : [500, 750, 1000, 1250, 1500]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "lsvc_grid_search = GridSearchCV(\n",
    "    estimator=LinearSVC(), \n",
    "    param_grid=lsvc_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), \n",
    "    n_jobs=-1,\n",
    "    \n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "lsvc_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get results\n",
    "lsvc_results_df = pd.DataFrame(lsvc_grid_search.cv_results_)\n",
    "\n",
    "# display top 10 models\n",
    "#lsvc_top10 = lsvc_results_df.sort_values(by='mean_test_score', ascending=True).head(10)\n",
    "\n",
    "# store best model\n",
    "lsvc_best_params_gs = lsvc_grid_search.best_params_\n",
    "lsvc_best_cv_score_gs = lsvc_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "923f62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Grid Search\n",
    "\n",
    "# define paramter space\n",
    "dt_param_grid = {\n",
    "    'criterion'         : ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter'          : ['best', 'random'],\n",
    "    'max_depth'         : [10, 25, 50, 100, 150, None],\n",
    "    'min_samples_split' : [2, 6, 10, 15, 20],\n",
    "    'min_samples_leaf'  : [1, 25, 50, 100, 150],\n",
    "    'max_features'      : [20, 50, 150, 300, 500, None]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "dt_grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(), \n",
    "    param_grid=dt_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get results\n",
    "dt_results_df = pd.DataFrame(dt_grid_search.cv_results_)\n",
    "\n",
    "# display top 10 models\n",
    "#dt_top10 = dt_results_df.sort_values(by='mean_test_score', ascending=True).head(10)\n",
    "\n",
    "# store best model\n",
    "dt_best_params_gs = dt_grid_search.best_params_\n",
    "dt_best_cv_score_gs = dt_grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9221e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Grid Search\n",
    "\n",
    "# define paramter space\n",
    "rf_param_grid = {\n",
    "    'criterion'         : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth'         : [2, 4, 10, 30, 100, None],\n",
    "    'min_samples_split' : [2, 6, 10, 15, 20],\n",
    "    'min_samples_leaf'  : [1, 25, 50, 100, 150],\n",
    "    'max_features'      : [20, 50, 150, 300, 500, None]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(), \n",
    "    param_grid=rf_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get results\n",
    "rf_results_df = pd.DataFrame(rf_grid_search.cv_results_)\n",
    "\n",
    "# display top 10 models\n",
    "#rf_top10 = rf_results_df.sort_values(by='mean_test_score', ascending=True).head(10)\n",
    "\n",
    "# store best model\n",
    "rf_best_params_gs = rf_grid_search.best_params_\n",
    "rf_best_cv_score_gs = rf_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94bd0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier Grid Search\n",
    "\n",
    "# define paramter space\n",
    "gbc_param_grid = {\n",
    "    'loss'              : ['log_loss', 'exponential'],\n",
    "    'n_estimators'      : [50, 100, 200, 500],\n",
    "    'min_samples_split' : [2, 6, 10, 15, 20],\n",
    "    'min_samples_leaf'  : [1, 25, 50, 100, 150],\n",
    "    'max_depth'         : [2, 3, 5, 10, 30, None]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "gbc_grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingClassifier(), \n",
    "    param_grid=gbc_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "gbc_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get results\n",
    "gbc_results_df = pd.DataFrame(gbc_grid_search.cv_results_)\n",
    "\n",
    "# display top 10 models\n",
    "#gbc_top10 = gbc_results_df.sort_values(by='mean_test_score', ascending=True).head(10)\n",
    "\n",
    "# store best model\n",
    "gbc_best_params_gs = gbc_grid_search.best_params_\n",
    "gbc_best_cv_score_gs = gbc_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49e2c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Classifier Grid Search\n",
    "\n",
    "# define paramter space\n",
    "knn_param_grid = {\n",
    "    'n_neighbors'       : [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'weights'           : ['uniform', 'distance'],\n",
    "    'algorithm'         : ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'p'                 : [1, 2]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "knn_grid_search = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(), \n",
    "    param_grid=knn_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5), \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "knn_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get results\n",
    "knn_results_df = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "# display top 10 models\n",
    "#knn_top10 = knn_results_df.sort_values(by='mean_test_score', ascending=True).head(10)\n",
    "\n",
    "# store best model\n",
    "knn_best_params_gs = knn_grid_search.best_params_\n",
    "knn_best_cv_score_gs = knn_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fdcdbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary:\n",
      "\n",
      "Logistic regression baseline score: 0.8103870777110214\n",
      "Logistic regression optuna best score: 0.8151718703831381\n",
      "Logistic regression GridSearchCV best score: 0.8100817492366787\n",
      "Logistic regression optuna best parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.2519187354950386, 'max_iter': 806}\n",
      "Logistic regression GridSearch CV best parameters: {'C': 0.75, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Linear support vector classifier baseline score: 0.8050586033684624\n",
      "Linear support vector classifier optuna best score: 0.8115158081355264\n",
      "Linear support vector classifier GridSearchCV best score: 0.8087245149216981\n",
      "Linear support vector classifier optuna best parameters: {'penalty': 'l2', 'C': 0.32068244899996284, 'max_iter': 913}\n",
      "Linear support vector classifier GridSearch CV best parameters: {'C': 0.25, 'max_iter': 500, 'penalty': 'l2'}\n",
      "Decision tree baseline score: 0.7783610755441741\n",
      "Decision tree optuna best score: 0.8112124495223086\n",
      "Decision tree GridSearchCV best score: 0.813487639121442\n",
      "Decision tree optuna best parameters: {'criterion': 'gini', 'splitter': 'best', 'max_depth': 43, 'min_samples_split': 12, 'min_samples_leaf': 38, 'max_features': 10}\n",
      "Decision tree GridSearch CV best parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'random'}\n",
      "Random forest baseline score: 0.7955244755244755\n",
      "Random forest optuna best score: 0.8300502314586821\n",
      "Random forest GridSearchCV best score: 0.824999507534719\n",
      "Random forest optuna best parameters: {'criterion': 'log_loss', 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 7}\n",
      "Random forest GridSearch CV best parameters: {'criterion': 'log_loss', 'max_depth': 100, 'max_features': 150, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Gradient boosting classifier baseline score: 0.8216586230670738\n",
      "Gradient boosting classifier optuna best score: 0.825249679897567\n",
      "Gradient boosting classifier GridSearchCV best score: 0.8303299517384025\n",
      "Gradient boosting classifier optuna best parameters: {'loss': 'log_loss', 'max_depth': 64, 'min_samples_split': 49, 'min_samples_leaf': 75, 'n_estimators': 413}\n",
      "Gradient boosting classifier GridSearch CV best parameters: {'loss': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 100}\n",
      "K-nearest neighbors baseline score: 0.7118034078597459\n",
      "K-nearest neighbors optuna best score: 0.762090022653403\n",
      "K-nearest neighbors GridSearchCV best score: 0.7499911356249385\n",
      "K-nearest neighbors optuna best parameters: {'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1, 'n_neighbors': 14}\n",
      "K-nearest neighbors GridSearch CV best parameters: {'algorithm': 'ball_tree', 'n_neighbors': 6, 'p': 1, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results\n",
    "\n",
    "print(\"Results summary:\\n\")\n",
    "\n",
    "print(f\"Logistic regression baseline score: {baseline_logistic_regression_accs.mean()}\")\n",
    "print(f\"Logistic regression optuna best score: {lr_best_cv_score}\")\n",
    "print(f\"Logistic regression GridSearchCV best score: {lr_best_cv_score_gs}\")\n",
    "print(f\"Logistic regression optuna best parameters: {lr_best_params}\")\n",
    "print(f\"Logistic regression GridSearch CV best parameters: {lr_best_params_gs}\")\n",
    "\n",
    "print(f\"Linear support vector classifier baseline score: {baseline_lsvc_accs.mean()}\")\n",
    "print(f\"Linear support vector classifier optuna best score: {lsvc_best_cv_score}\")\n",
    "print(f\"Linear support vector classifier GridSearchCV best score: {lsvc_best_cv_score_gs}\")\n",
    "print(f\"Linear support vector classifier optuna best parameters: {lsvc_best_params}\")\n",
    "print(f\"Linear support vector classifier GridSearch CV best parameters: {lsvc_best_params_gs}\")\n",
    "\n",
    "print(f\"Decision tree baseline score: {baseline_decision_tree_accs.mean()}\")\n",
    "print(f\"Decision tree optuna best score: {dt_best_cv_score}\")\n",
    "print(f\"Decision tree GridSearchCV best score: {dt_best_cv_score_gs}\")\n",
    "print(f\"Decision tree optuna best parameters: {dt_best_params}\")\n",
    "print(f\"Decision tree GridSearch CV best parameters: {dt_best_params_gs}\")\n",
    "\n",
    "print(f\"Random forest baseline score: {baseline_random_forest_accs.mean()}\")\n",
    "print(f\"Random forest optuna best score: {rf_best_cv_score}\")\n",
    "print(f\"Random forest GridSearchCV best score: {rf_best_cv_score_gs}\")\n",
    "print(f\"Random forest optuna best parameters: {rf_best_params}\")\n",
    "print(f\"Random forest GridSearch CV best parameters: {rf_best_params_gs}\")\n",
    "\n",
    "print(f\"Gradient boosting classifier baseline score: {baseline_gbc_accs.mean()}\")\n",
    "print(f\"Gradient boosting classifier optuna best score: {gbc_best_cv_score}\")\n",
    "print(f\"Gradient boosting classifier GridSearchCV best score: {gbc_best_cv_score_gs}\")\n",
    "print(f\"Gradient boosting classifier optuna best parameters: {gbc_best_params}\")\n",
    "print(f\"Gradient boosting classifier GridSearch CV best parameters: {gbc_best_params_gs}\")\n",
    "\n",
    "print(f\"K-nearest neighbors baseline score: {baseline_knn_accs.mean()}\")\n",
    "print(f\"K-nearest neighbors optuna best score: {knn_best_cv_score}\")\n",
    "print(f\"K-nearest neighbors GridSearchCV best score: {knn_best_cv_score_gs}\")\n",
    "print(f\"K-nearest neighbors optuna best parameters: {knn_best_params}\")\n",
    "print(f\"K-nearest neighbors GridSearch CV best parameters: {knn_best_params_gs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1cecf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       892\n",
      "1       893\n",
      "2       894\n",
      "3       895\n",
      "4       896\n",
      "       ... \n",
      "413    1305\n",
      "414    1306\n",
      "415    1307\n",
      "416    1308\n",
      "417    1309\n",
      "Name: PassengerId, Length: 418, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc93aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define final models\n",
    "lr_final_model = LogisticRegression()\n",
    "lr_final_model.set_params(**lr_best_params)\n",
    "\n",
    "lsvc_final_model = LinearSVC()\n",
    "lsvc_final_model.set_params(**lsvc_best_params)\n",
    "\n",
    "dt_final_model = DecisionTreeClassifier()\n",
    "dt_final_model.set_params(**dt_best_params) #not choosing grid search despite higher cv score due to overfitting concerns\n",
    "\n",
    "rf_final_model = RandomForestClassifier()\n",
    "rf_final_model.set_params(**rf_best_params)\n",
    "\n",
    "gbc_final_model = GradientBoostingClassifier()\n",
    "gbc_final_model.set_params(**gbc_best_params_gs)\n",
    "\n",
    "knn_final_model = KNeighborsClassifier()\n",
    "knn_final_model.set_params(**knn_best_params)\n",
    "\n",
    "#fit models\n",
    "lr_final_model.fit(X_train, y_train)\n",
    "lsvc_final_model.fit(X_train, y_train)\n",
    "dt_final_model.fit(X_train, y_train)\n",
    "rf_final_model.fit(X_train, y_train)\n",
    "gbc_final_model.fit(X_train, y_train)\n",
    "knn_final_model.fit(X_train, y_train)\n",
    "\n",
    "#predict on final dataset\n",
    "lr_pred =   lr_final_model.predict(X_final)\n",
    "lsvc_pred = lsvc_final_model.predict(X_final)\n",
    "dt_pred =   dt_final_model.predict(X_final)\n",
    "rf_pred =   rf_final_model.predict(X_final)\n",
    "gbc_pred =  gbc_final_model.predict(X_final)\n",
    "knn_pred =  knn_final_model.predict(X_final)\n",
    "\n",
    "#define submission data\n",
    "lr_data =   {'PassengerId' : pid, 'Survived' : lr_pred}\n",
    "lsvc_data = {'PassengerId' : pid, 'Survived' : lsvc_pred}\n",
    "dt_data =   {'PassengerId' : pid, 'Survived' : dt_pred}\n",
    "rf_data =   {'PassengerId' : pid, 'Survived' : rf_pred}\n",
    "gbc_data =  {'PassengerId' : pid, 'Survived' : gbc_pred}\n",
    "knn_data =  {'PassengerId' : pid, 'Survived' : knn_pred}\n",
    "lr_submission =     pd.DataFrame(lr_data)\n",
    "lsvc_submission =   pd.DataFrame(lsvc_data)\n",
    "dt_submission =     pd.DataFrame(dt_data)\n",
    "rf_submission =     pd.DataFrame(rf_data)\n",
    "gbc_submission =    pd.DataFrame(gbc_data)\n",
    "knn_submission =    pd.DataFrame(knn_data)\n",
    "\n",
    "lr_submission.to_csv('logistic_regression_submission.csv', index=False)\n",
    "lsvc_submission.to_csv('linear_svc_submission.csv', index=False)\n",
    "dt_submission.to_csv('decision_tree_submission.csv', index=False)\n",
    "rf_submission.to_csv('random_forest_submission.csv', index=False)\n",
    "gbc_submission.to_csv('gradient_boosting_classifier_submission.csv', index=False)\n",
    "knn_submission.to_csv('knearest_neighbors_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b139ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create voting classifier from all of the models \n",
    "\n",
    "lr_best_params = {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.2519187354950386, 'max_iter': 806}\n",
    "lsvc_best_params = {'penalty': 'l2', 'C': 0.32068244899996284, 'max_iter': 913}\n",
    "dt_best_params = {'criterion': 'gini', 'splitter': 'best', 'max_depth': 43, 'min_samples_split': 12, 'min_samples_leaf': 38, 'max_features': 10}\n",
    "rf_best_params = {'criterion': 'log_loss', 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 7}\n",
    "gbc_best_params = {'loss': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 100}\n",
    "knn_best_params = {'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1, 'n_neighbors': 14}\n",
    "\n",
    "lr_voting_model = LogisticRegression()\n",
    "lr_voting_model.set_params(**lr_best_params)\n",
    "\n",
    "lsvc_voting_model = LinearSVC()\n",
    "lsvc_voting_model.set_params(**lsvc_best_params)\n",
    "\n",
    "dt_voting_model = DecisionTreeClassifier()\n",
    "dt_voting_model.set_params(**dt_best_params) #not choosing grid search despite higher cv score due to overfitting concerns\n",
    "\n",
    "rf_voting_model = RandomForestClassifier()\n",
    "rf_voting_model.set_params(**rf_best_params)\n",
    "\n",
    "gbc_voting_model = GradientBoostingClassifier()\n",
    "gbc_voting_model.set_params(**gbc_best_params)\n",
    "\n",
    "knn_voting_model = KNeighborsClassifier()\n",
    "knn_voting_model.set_params(**knn_best_params)\n",
    "\n",
    "#create voting classifiers\n",
    "hard_voter = VotingClassifier(estimators=[('lr', lr_voting_model), \n",
    "                                     ('dt', dt_voting_model), \n",
    "                                     ('rf', rf_voting_model), \n",
    "                                     ('gbc', gbc_voting_model), \n",
    "                                     ('knn', knn_voting_model)], voting='hard')\n",
    "hard_voter.fit(X_train, y_train)\n",
    "\n",
    "soft_voter = VotingClassifier(estimators=[('lr', lr_voting_model),\n",
    "                                     ('dt', dt_voting_model), \n",
    "                                     ('rf', rf_voting_model), \n",
    "                                     ('gbc', gbc_voting_model), \n",
    "                                     ('knn', knn_voting_model)], voting='soft')\n",
    "soft_voter.fit(X_train, y_train)\n",
    "\n",
    "#use voters to predict\n",
    "hard_pred = hard_voter.predict(X_final)\n",
    "soft_pred = soft_voter.predict(X_final)\n",
    "\n",
    "#create submission files\n",
    "hard_data = {'PassengerId' : pid, 'Survived' : hard_pred}\n",
    "soft_data = {'PassengerId' : pid, 'Survived' : soft_pred}\n",
    "\n",
    "hard_submission = pd.DataFrame(hard_data)\n",
    "soft_submission = pd.DataFrame(soft_data)\n",
    "\n",
    "hard_submission.to_csv('hard_voting_classifier_submission.csv', index=False)\n",
    "soft_submission.to_csv('soft_voting_classifier_submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
